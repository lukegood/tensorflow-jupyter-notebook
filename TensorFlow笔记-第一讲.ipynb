{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码、图片均来源于中国大学MOOC曹健老师《人工智能实践：TensorFlow笔记2》，地址：https://www.icourse163.org/learn/PKU-1002536002?tid=1452937471#/learn/announce ，部分代码在我练习过程中有修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 5], shape=(2,), dtype=int64)\n",
      "<dtype: 'int64'>\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1, 5], dtype = tf.int64)\n",
    "print(a)\n",
    "print(a.dtype)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> shape中逗号隔开几个数字，就是几维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(0, 5)\n",
    "b = tf.convert_to_tensor(a, dtype = tf.int64)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> tf.convert_to_tensor将numpy格式转为张量，两个参数分别是数据名和类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何创建一个张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0.]\n",
      " [0. 0.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[9 9 9]\n",
      " [9 9 9]\n",
      " [9 9 9]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.zeros([2, 2])\n",
    "b = tf.ones(4)\n",
    "c = tf.fill([3,3], 9)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.44468048 0.5114101 ]\n",
      " [0.3226295  2.6347578 ]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.3241434  1.270691  ]\n",
      " [0.35826704 0.04361027]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal([2, 2], mean = 0.5, stddev = 1)\n",
    "b = tf.random.truncated_normal([2, 2], mean = 0.5, stddev = 1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> random.normal的参数分别是维度，均值，标准差。random.truncated_normal可以保证生成的随机数在正负2个标准差之内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.11616158 0.1039319 ]\n",
      " [0.87229264 0.4636017 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.uniform([2, 2], minval = 0, maxval = 1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> random.uniform的参数分别是维度，最小值，最大值，生成一个左闭右开区间的服从均匀分布的指定维度的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.constant([1, 2, 3], dtype = tf.int64)\n",
    "x2 = tf.cast(x1, dtype = tf.int32)\n",
    "print(tf.reduce_min(x2), tf.reduce_max(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> cast表示强制类型转换，参数分别是张量名和要转为的类型。\n",
    "\n",
    "> reduce_min找出向量所有元素里的最小值，max找出最大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 3 4], shape=(3,), dtype=int64) tf.Tensor([ 6 15], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1, 2, 3], [4, 5, 6]], dtype = tf.int64)\n",
    "x1 = tf.reduce_mean(x, axis = 0)\n",
    "x2 = tf.reduce_sum(x, axis = 1)\n",
    "print(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> axis指定操作方向。axis等于几，表示对哪个维度进行操作。对于二维数组来说，axis=0表示经度方向（对第一个维度操作，也就是跨行），axis=1表示纬度方向（对第二维操作，也就是跨列）。\n",
    "\n",
    "> reduce_mean和sum分别表示求平均值和和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=int64, numpy=5>\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(tf.constant(5, dtype = tf.int64))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Variable可以将张量标记为可训练的，会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记训练参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[8. 8. 8.]], shape=(1, 3), dtype=float32) tf.Tensor([[-6. -6. -6.]], shape=(1, 3), dtype=float32) tf.Tensor([[7. 7. 7.]], shape=(1, 3), dtype=float32) tf.Tensor([[0.14285715 0.14285715 0.14285715]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones([1, 3])\n",
    "y = tf.fill([1, 3], 7.)\n",
    "r1 = tf.add(x, y)\n",
    "r2 = tf.subtract(x, y)\n",
    "r3 = tf.multiply(x, y)\n",
    "r4 = tf.divide(x, y)\n",
    "print(r1, r2, r3, r4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[49. 49. 49.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[49. 49. 49.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[2.6457512 2.6457512 2.6457512]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.square(y))\n",
    "print(tf.pow(y, 2))\n",
    "print(tf.sqrt(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[168.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "z = tf.fill([3, 1], 8.)\n",
    "print(tf.matmul(y, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((), ()), types: (tf.int32, tf.int32)>\n",
      "(<tf.Tensor: id=106, shape=(), dtype=int32, numpy=12>, <tf.Tensor: id=107, shape=(), dtype=int32, numpy=0>)\n",
      "(<tf.Tensor: id=108, shape=(), dtype=int32, numpy=15>, <tf.Tensor: id=109, shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: id=110, shape=(), dtype=int32, numpy=17>, <tf.Tensor: id=111, shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: id=112, shape=(), dtype=int32, numpy=10>, <tf.Tensor: id=113, shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([12, 15, 17,10])\n",
    "label = tf.constant([0, 1, 1, 0])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, label))\n",
    "print(dataset)\n",
    "for e in dataset:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from_tensor_slices用于给特征和标签配对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    w = tf.Variable(tf.constant(3.0))\n",
    "    loss = tf.pow(w, 2)\n",
    "grad = tape.gradient(loss, w)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with结构记录运算过程，gradient计算张量的梯度。\n",
    "\n",
    "with tf.GradientTape as tape:\n",
    "\n",
    "   多个计算过程\n",
    "    \n",
    "grad = tape.gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 one\n",
      "1 two\n",
      "2 three\n"
     ]
    }
   ],
   "source": [
    "l = ['one', 'two', 'three']\n",
    "for i, element in enumerate(l):\n",
    "    print(i, element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enumerate是Python内建函数，可以便利每一个元素，组合为：索引 元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classes = 3\n",
    "label = tf.constant([0, 1, 2])\n",
    "output = tf.one_hot(label, depth = classes)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one_hot可以将数据转为独热码的形式。tf.one_hot(待转换的数据，depth=几分类)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.25598174 0.69583046 0.04818781], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y = tf.constant([1.01, 2.01, -0.66])\n",
    "y_pro = tf.nn.softmax(y)\n",
    "print(y_pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当n分类的n个输出（y0，...，yn）通过Softmax（）函数，便符合概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.wangliguang.cn/wp-content/uploads/2020/03/批注-2020-03-12-083246.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=4>\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(5)\n",
    "a.assign_sub(1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign_sub用于自减1操作，要求该变量必须是可训练的（先用tf.Variable定义好）。用法：assing_sub（要自减的内容）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3 3 3], shape=(3,), dtype=int64)\n",
      "tf.Tensor([2 2 2 2], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "m = tf.constant([[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6]], dtype = tf.int32)\n",
    "b = tf.argmax(m, axis = 0)\n",
    "c = tf.argmax(m, axis = 1)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.argmax返回指定维度上最大值的索引号，维度用axis指定，意义与之前reduce_sum等相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from pandas import DataFrame\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "     花萼长  花萼宽  花瓣长  花瓣宽\n",
      "0       5.1     3.5     1.4     0.2\n",
      "1       4.9     3.0     1.4     0.2\n",
      "2       4.7     3.2     1.3     0.2\n",
      "3       4.6     3.1     1.5     0.2\n",
      "4       5.0     3.6     1.4     0.2\n",
      "..      ...     ...     ...     ...\n",
      "145     6.7     3.0     5.2     2.3\n",
      "146     6.3     2.5     5.0     1.9\n",
      "147     6.5     3.0     5.2     2.0\n",
      "148     6.2     3.4     5.4     2.3\n",
      "149     5.9     3.0     5.1     1.8\n",
      "\n",
      "[150 rows x 4 columns]\n",
      "     花萼长  花萼宽  花瓣长  花瓣宽  类别\n",
      "0       5.1     3.5     1.4     0.2     0\n",
      "1       4.9     3.0     1.4     0.2     0\n",
      "2       4.7     3.2     1.3     0.2     0\n",
      "3       4.6     3.1     1.5     0.2     0\n",
      "4       5.0     3.6     1.4     0.2     0\n",
      "..      ...     ...     ...     ...   ...\n",
      "145     6.7     3.0     5.2     2.3     2\n",
      "146     6.3     2.5     5.0     1.9     2\n",
      "147     6.5     3.0     5.2     2.0     2\n",
      "148     6.2     3.4     5.4     2.3     2\n",
      "149     5.9     3.0     5.1     1.8     2\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "x_data = datasets.load_iris().data\n",
    "y_data = datasets.load_iris().target\n",
    "print(x_data)\n",
    "print(y_data)\n",
    "\n",
    "x_data = DataFrame(x_data, columns = ['花萼长', '花萼宽', '花瓣长', '花瓣宽'])\n",
    "pd.set_option('display.unicode.east_asian_width', True)\n",
    "print(x_data)\n",
    "\n",
    "x_data['类别'] = y_data\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4934ff67e2f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_iris\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_iris\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m116\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "x_data = datasets.load_iris().data\n",
    "y_data = datasets.load_iris().target\n",
    "\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(x_data)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y_data)\n",
    "tf.random.set_seed(116)\n",
    "\n",
    "x_train = x_data[:-30]\n",
    "y_train = y_data[:-30]\n",
    "x_test = x_data[-30:]\n",
    "y_test = y_data[-30:]\n",
    "\n",
    "x_train = tf.cast(x_train, dtype = tf.float32)\n",
    "x_test = tf.cast(x_test, dtype = tf.float32)\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev = 0.1, seed = 1))\n",
    "b1 = tf.Variable(tf.random.truncated_normal([3], stddev = 0.1, seed = 1))\n",
    "                 \n",
    "lr = 0.1\n",
    "train_loss_results = []\n",
    "test_acc = []\n",
    "epoch = 500\n",
    "loss_all = 0\n",
    "                 \n",
    "for epoch in range(epoch):\n",
    "    for step, (x_train, y_train) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = tf.matmul(x_train, w1) + b1\n",
    "            y = tf.nn.softmax(y)\n",
    "            y_ = tf.one_hot(y_train, depth = 3)\n",
    "            loss = tf.reduce_mean(tf.square(y_-y))\n",
    "            loss_all += loss.numpy()\n",
    "                                  \n",
    "        grads = tape.gradient(loss, [w1, b1])\n",
    "        print(x_train)\n",
    "        w1.assign_sub(lr * grads[0])\n",
    "        b1.assign_sub(lr * grads[1])\n",
    "                                  \n",
    "    print(\"Epoch.{}, loss:{}\".format(epoch, loss_all/4))\n",
    "    train_loss_results.append(loss_all/4)\n",
    "    loss_all = 0\n",
    "                                  \n",
    "    total_correct, total_number = 0, 0\n",
    "    for x_text, y_test in test_db:\n",
    "        y = tf.matmul(x_test, w1) + b1\n",
    "        y = tf.nn.softmax(y)\n",
    "        pred = tf.argmax(y, axis = 1)\n",
    "        pred = tf.cast(pred, dtype = y_test.dtype)\n",
    "        correct = tf.cast(tf.equal(pred, y_test), dtype = tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        total_correct += int(correct)\n",
    "        total_number += x_test.shape[0]\n",
    "    acc = total_correct/total_number\n",
    "    test_acc.append(acc)\n",
    "    print(\"test_acc:\", test_acc)\n",
    "    \n",
    "    # 绘制 loss 曲线\n",
    "plt.title('Loss Function Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Loss')  # y轴变量名称\n",
    "plt.plot(train_loss_results, label=\"$Loss$\")  # 逐点画出trian_loss_results值并连线，连线图标是Loss\n",
    "plt.legend()  # 画出曲线图标\n",
    "plt.show()  # 画出图像\n",
    "\n",
    "# 绘制 Accuracy 曲线\n",
    "plt.title('Acc Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Acc')  # y轴变量名称\n",
    "plt.plot(test_acc, label=\"$Accuracy$\")  # 逐点画出test_acc值并连线，连线图标是Accuracy\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码练习："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss:0.282131090760231\n",
      "acc: 0.16666666666666666\n",
      "Epoch 1, loss:0.25459615513682365\n",
      "acc: 0.16666666666666666\n",
      "Epoch 2, loss:0.22570250183343887\n",
      "acc: 0.16666666666666666\n",
      "Epoch 3, loss:0.21028400212526321\n",
      "acc: 0.16666666666666666\n",
      "Epoch 4, loss:0.19942264631390572\n",
      "acc: 0.16666666666666666\n",
      "Epoch 5, loss:0.18873637914657593\n",
      "acc: 0.5\n",
      "Epoch 6, loss:0.17851299419999123\n",
      "acc: 0.5333333333333333\n",
      "Epoch 7, loss:0.16922875866293907\n",
      "acc: 0.5333333333333333\n",
      "Epoch 8, loss:0.16107673943042755\n",
      "acc: 0.5333333333333333\n",
      "Epoch 9, loss:0.15404684469103813\n",
      "acc: 0.5333333333333333\n",
      "Epoch 10, loss:0.14802726358175278\n",
      "acc: 0.5333333333333333\n",
      "Epoch 11, loss:0.14287303388118744\n",
      "acc: 0.5333333333333333\n",
      "Epoch 12, loss:0.1384414117783308\n",
      "acc: 0.5333333333333333\n",
      "Epoch 13, loss:0.13460606522858143\n",
      "acc: 0.5333333333333333\n",
      "Epoch 14, loss:0.13126072846353054\n",
      "acc: 0.5333333333333333\n",
      "Epoch 15, loss:0.12831821851432323\n",
      "acc: 0.5333333333333333\n",
      "Epoch 16, loss:0.12570795230567455\n",
      "acc: 0.5333333333333333\n",
      "Epoch 17, loss:0.12337298691272736\n",
      "acc: 0.5333333333333333\n",
      "Epoch 18, loss:0.12126746587455273\n",
      "acc: 0.5333333333333333\n",
      "Epoch 19, loss:0.11935433372855186\n",
      "acc: 0.5333333333333333\n",
      "Epoch 20, loss:0.11760355159640312\n",
      "acc: 0.5333333333333333\n",
      "Epoch 21, loss:0.11599067971110344\n",
      "acc: 0.5333333333333333\n",
      "Epoch 22, loss:0.11449567973613739\n",
      "acc: 0.5333333333333333\n",
      "Epoch 23, loss:0.11310207843780518\n",
      "acc: 0.5333333333333333\n",
      "Epoch 24, loss:0.11179621517658234\n",
      "acc: 0.5333333333333333\n",
      "Epoch 25, loss:0.11056671477854252\n",
      "acc: 0.5333333333333333\n",
      "Epoch 26, loss:0.10940408147871494\n",
      "acc: 0.5333333333333333\n",
      "Epoch 27, loss:0.10830028168857098\n",
      "acc: 0.5333333333333333\n",
      "Epoch 28, loss:0.10724855028092861\n",
      "acc: 0.5333333333333333\n",
      "Epoch 29, loss:0.10624313540756702\n",
      "acc: 0.5333333333333333\n",
      "Epoch 30, loss:0.10527910105884075\n",
      "acc: 0.5333333333333333\n",
      "Epoch 31, loss:0.10435221903026104\n",
      "acc: 0.5333333333333333\n",
      "Epoch 32, loss:0.10345886647701263\n",
      "acc: 0.5333333333333333\n",
      "Epoch 33, loss:0.10259587317705154\n",
      "acc: 0.5333333333333333\n",
      "Epoch 34, loss:0.10176053084433079\n",
      "acc: 0.5333333333333333\n",
      "Epoch 35, loss:0.10095042362809181\n",
      "acc: 0.5333333333333333\n",
      "Epoch 36, loss:0.10016347095370293\n",
      "acc: 0.5333333333333333\n",
      "Epoch 37, loss:0.09939784742891788\n",
      "acc: 0.5333333333333333\n",
      "Epoch 38, loss:0.0986519306898117\n",
      "acc: 0.5333333333333333\n",
      "Epoch 39, loss:0.09792428836226463\n",
      "acc: 0.5333333333333333\n",
      "Epoch 40, loss:0.09721365012228489\n",
      "acc: 0.5333333333333333\n",
      "Epoch 41, loss:0.09651889465749264\n",
      "acc: 0.5333333333333333\n",
      "Epoch 42, loss:0.09583901055157185\n",
      "acc: 0.5333333333333333\n",
      "Epoch 43, loss:0.09517310559749603\n",
      "acc: 0.5333333333333333\n",
      "Epoch 44, loss:0.09452036768198013\n",
      "acc: 0.5333333333333333\n",
      "Epoch 45, loss:0.09388007409870625\n",
      "acc: 0.5333333333333333\n",
      "Epoch 46, loss:0.09325156360864639\n",
      "acc: 0.5333333333333333\n",
      "Epoch 47, loss:0.09263424947857857\n",
      "acc: 0.5333333333333333\n",
      "Epoch 48, loss:0.09202759712934494\n",
      "acc: 0.5333333333333333\n",
      "Epoch 49, loss:0.09143111854791641\n",
      "acc: 0.5333333333333333\n",
      "Epoch 50, loss:0.09084436483681202\n",
      "acc: 0.5666666666666667\n",
      "Epoch 51, loss:0.09026693552732468\n",
      "acc: 0.5666666666666667\n",
      "Epoch 52, loss:0.08969846926629543\n",
      "acc: 0.5666666666666667\n",
      "Epoch 53, loss:0.08913860656321049\n",
      "acc: 0.6\n",
      "Epoch 54, loss:0.08858706057071686\n",
      "acc: 0.6\n",
      "Epoch 55, loss:0.08804351836442947\n",
      "acc: 0.6\n",
      "Epoch 56, loss:0.08750772476196289\n",
      "acc: 0.6\n",
      "Epoch 57, loss:0.08697943761944771\n",
      "acc: 0.6\n",
      "Epoch 58, loss:0.08645843341946602\n",
      "acc: 0.6\n",
      "Epoch 59, loss:0.08594449236989021\n",
      "acc: 0.6\n",
      "Epoch 60, loss:0.08543740771710873\n",
      "acc: 0.6\n",
      "Epoch 61, loss:0.08493702299892902\n",
      "acc: 0.6\n",
      "Epoch 62, loss:0.08444313891232014\n",
      "acc: 0.6333333333333333\n",
      "Epoch 63, loss:0.08395560085773468\n",
      "acc: 0.6333333333333333\n",
      "Epoch 64, loss:0.08347426354885101\n",
      "acc: 0.6333333333333333\n",
      "Epoch 65, loss:0.0829989816993475\n",
      "acc: 0.6333333333333333\n",
      "Epoch 66, loss:0.08252961561083794\n",
      "acc: 0.6333333333333333\n",
      "Epoch 67, loss:0.08206604979932308\n",
      "acc: 0.6333333333333333\n",
      "Epoch 68, loss:0.0816081315279007\n",
      "acc: 0.6333333333333333\n",
      "Epoch 69, loss:0.08115578256547451\n",
      "acc: 0.6333333333333333\n",
      "Epoch 70, loss:0.08070887438952923\n",
      "acc: 0.6333333333333333\n",
      "Epoch 71, loss:0.08026731573045254\n",
      "acc: 0.6333333333333333\n",
      "Epoch 72, loss:0.07983098737895489\n",
      "acc: 0.6666666666666666\n",
      "Epoch 73, loss:0.07939981482923031\n",
      "acc: 0.6666666666666666\n",
      "Epoch 74, loss:0.07897369377315044\n",
      "acc: 0.6666666666666666\n",
      "Epoch 75, loss:0.07855254225432873\n",
      "acc: 0.7\n",
      "Epoch 76, loss:0.07813627645373344\n",
      "acc: 0.7\n",
      "Epoch 77, loss:0.07772481441497803\n",
      "acc: 0.7\n",
      "Epoch 78, loss:0.07731806859374046\n",
      "acc: 0.7\n",
      "Epoch 79, loss:0.07691597752273083\n",
      "acc: 0.7\n",
      "Epoch 80, loss:0.07651844993233681\n",
      "acc: 0.7\n",
      "Epoch 81, loss:0.07612544111907482\n",
      "acc: 0.7333333333333333\n",
      "Epoch 82, loss:0.07573685981333256\n",
      "acc: 0.7333333333333333\n",
      "Epoch 83, loss:0.07535265386104584\n",
      "acc: 0.7333333333333333\n",
      "Epoch 84, loss:0.07497275061905384\n",
      "acc: 0.7333333333333333\n",
      "Epoch 85, loss:0.07459708396345377\n",
      "acc: 0.7666666666666667\n",
      "Epoch 86, loss:0.07422559522092342\n",
      "acc: 0.7666666666666667\n",
      "Epoch 87, loss:0.07385822758078575\n",
      "acc: 0.7666666666666667\n",
      "Epoch 88, loss:0.07349492143839598\n",
      "acc: 0.7666666666666667\n",
      "Epoch 89, loss:0.07313561998307705\n",
      "acc: 0.7666666666666667\n",
      "Epoch 90, loss:0.07278026267886162\n",
      "acc: 0.7666666666666667\n",
      "Epoch 91, loss:0.07242879923433065\n",
      "acc: 0.7666666666666667\n",
      "Epoch 92, loss:0.07208117935806513\n",
      "acc: 0.7666666666666667\n",
      "Epoch 93, loss:0.07173734065145254\n",
      "acc: 0.8\n",
      "Epoch 94, loss:0.07139723934233189\n",
      "acc: 0.8\n",
      "Epoch 95, loss:0.07106082141399384\n",
      "acc: 0.8\n",
      "Epoch 96, loss:0.07072803657501936\n",
      "acc: 0.8\n",
      "Epoch 97, loss:0.07039884570986032\n",
      "acc: 0.8\n",
      "Epoch 98, loss:0.07007318269461393\n",
      "acc: 0.8333333333333334\n",
      "Epoch 99, loss:0.06975102238357067\n",
      "acc: 0.8666666666666667\n",
      "Epoch 100, loss:0.06943229679018259\n",
      "acc: 0.8666666666666667\n",
      "Epoch 101, loss:0.06911696959286928\n",
      "acc: 0.8666666666666667\n",
      "Epoch 102, loss:0.06880500819534063\n",
      "acc: 0.8666666666666667\n",
      "Epoch 103, loss:0.0684963557869196\n",
      "acc: 0.8666666666666667\n",
      "Epoch 104, loss:0.06819096393883228\n",
      "acc: 0.8666666666666667\n",
      "Epoch 105, loss:0.06788880191743374\n",
      "acc: 0.8666666666666667\n",
      "Epoch 106, loss:0.06758982222527266\n",
      "acc: 0.8666666666666667\n",
      "Epoch 107, loss:0.06729398854076862\n",
      "acc: 0.9\n",
      "Epoch 108, loss:0.06700124870985746\n",
      "acc: 0.9\n",
      "Epoch 109, loss:0.06671156827360392\n",
      "acc: 0.9\n",
      "Epoch 110, loss:0.06642491277307272\n",
      "acc: 0.9\n",
      "Epoch 111, loss:0.06614124123007059\n",
      "acc: 0.9\n",
      "Epoch 112, loss:0.06586050800979137\n",
      "acc: 0.9\n",
      "Epoch 113, loss:0.0655826861038804\n",
      "acc: 0.9\n",
      "Epoch 114, loss:0.06530773360282183\n",
      "acc: 0.9\n",
      "Epoch 115, loss:0.06503561045974493\n",
      "acc: 0.9\n",
      "Epoch 116, loss:0.06476627476513386\n",
      "acc: 0.9\n",
      "Epoch 117, loss:0.06449970230460167\n",
      "acc: 0.9333333333333333\n",
      "Epoch 118, loss:0.06423586327582598\n",
      "acc: 0.9333333333333333\n",
      "Epoch 119, loss:0.0639747055247426\n",
      "acc: 0.9333333333333333\n",
      "Epoch 120, loss:0.06371620018035173\n",
      "acc: 0.9333333333333333\n",
      "Epoch 121, loss:0.06346031557768583\n",
      "acc: 0.9333333333333333\n",
      "Epoch 122, loss:0.06320702191442251\n",
      "acc: 0.9333333333333333\n",
      "Epoch 123, loss:0.06295627541840076\n",
      "acc: 0.9333333333333333\n",
      "Epoch 124, loss:0.0627080500125885\n",
      "acc: 0.9333333333333333\n",
      "Epoch 125, loss:0.062462314032018185\n",
      "acc: 0.9333333333333333\n",
      "Epoch 126, loss:0.062219045124948025\n",
      "acc: 0.9333333333333333\n",
      "Epoch 127, loss:0.06197819113731384\n",
      "acc: 0.9333333333333333\n",
      "Epoch 128, loss:0.061739733442664146\n",
      "acc: 0.9333333333333333\n",
      "Epoch 129, loss:0.06150364503264427\n",
      "acc: 0.9333333333333333\n",
      "Epoch 130, loss:0.06126988586038351\n",
      "acc: 0.9333333333333333\n",
      "Epoch 131, loss:0.06103843916207552\n",
      "acc: 0.9333333333333333\n",
      "Epoch 132, loss:0.060809262096881866\n",
      "acc: 0.9333333333333333\n",
      "Epoch 133, loss:0.06058233417570591\n",
      "acc: 0.9333333333333333\n",
      "Epoch 134, loss:0.06035762373358011\n",
      "acc: 0.9333333333333333\n",
      "Epoch 135, loss:0.06013511121273041\n",
      "acc: 0.9333333333333333\n",
      "Epoch 136, loss:0.05991475377231836\n",
      "acc: 0.9333333333333333\n",
      "Epoch 137, loss:0.059696528129279613\n",
      "acc: 0.9333333333333333\n",
      "Epoch 138, loss:0.059480417519807816\n",
      "acc: 0.9333333333333333\n",
      "Epoch 139, loss:0.05926638934761286\n",
      "acc: 0.9333333333333333\n",
      "Epoch 140, loss:0.0590544156730175\n",
      "acc: 0.9333333333333333\n",
      "Epoch 141, loss:0.058844469487667084\n",
      "acc: 0.9333333333333333\n",
      "Epoch 142, loss:0.05863652750849724\n",
      "acc: 0.9333333333333333\n",
      "Epoch 143, loss:0.058430567383766174\n",
      "acc: 0.9333333333333333\n",
      "Epoch 144, loss:0.058226561173796654\n",
      "acc: 0.9333333333333333\n",
      "Epoch 145, loss:0.058024486526846886\n",
      "acc: 0.9333333333333333\n",
      "Epoch 146, loss:0.05782431084662676\n",
      "acc: 0.9333333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147, loss:0.0576260257512331\n",
      "acc: 0.9333333333333333\n",
      "Epoch 148, loss:0.05742959678173065\n",
      "acc: 0.9333333333333333\n",
      "Epoch 149, loss:0.05723499786108732\n",
      "acc: 0.9333333333333333\n",
      "Epoch 150, loss:0.05704221688210964\n",
      "acc: 0.9333333333333333\n",
      "Epoch 151, loss:0.056851222179830074\n",
      "acc: 0.9333333333333333\n",
      "Epoch 152, loss:0.05666199419647455\n",
      "acc: 0.9333333333333333\n",
      "Epoch 153, loss:0.05647451616823673\n",
      "acc: 0.9333333333333333\n",
      "Epoch 154, loss:0.0562887629494071\n",
      "acc: 0.9333333333333333\n",
      "Epoch 155, loss:0.05610471498221159\n",
      "acc: 0.9333333333333333\n",
      "Epoch 156, loss:0.055922347120940685\n",
      "acc: 0.9333333333333333\n",
      "Epoch 157, loss:0.05574163980782032\n",
      "acc: 0.9333333333333333\n",
      "Epoch 158, loss:0.05556256417185068\n",
      "acc: 0.9333333333333333\n",
      "Epoch 159, loss:0.05538512207567692\n",
      "acc: 0.9333333333333333\n",
      "Epoch 160, loss:0.055209274403750896\n",
      "acc: 0.9333333333333333\n",
      "Epoch 161, loss:0.05503501370549202\n",
      "acc: 0.9333333333333333\n",
      "Epoch 162, loss:0.054862307384610176\n",
      "acc: 0.9333333333333333\n",
      "Epoch 163, loss:0.054691147059202194\n",
      "acc: 0.9333333333333333\n",
      "Epoch 164, loss:0.054521515034139156\n",
      "acc: 0.9666666666666667\n",
      "Epoch 165, loss:0.054353391751646996\n",
      "acc: 0.9666666666666667\n",
      "Epoch 166, loss:0.05418673623353243\n",
      "acc: 0.9666666666666667\n",
      "Epoch 167, loss:0.0540215615183115\n",
      "acc: 0.9666666666666667\n",
      "Epoch 168, loss:0.05385783780366182\n",
      "acc: 0.9666666666666667\n",
      "Epoch 169, loss:0.0536955501884222\n",
      "acc: 0.9666666666666667\n",
      "Epoch 170, loss:0.05353467632085085\n",
      "acc: 0.9666666666666667\n",
      "Epoch 171, loss:0.05337520595639944\n",
      "acc: 0.9666666666666667\n",
      "Epoch 172, loss:0.05321712139993906\n",
      "acc: 0.9666666666666667\n",
      "Epoch 173, loss:0.053060393780469894\n",
      "acc: 0.9666666666666667\n",
      "Epoch 174, loss:0.05290502216666937\n",
      "acc: 0.9666666666666667\n",
      "Epoch 175, loss:0.05275098606944084\n",
      "acc: 0.9666666666666667\n",
      "Epoch 176, loss:0.05259826127439737\n",
      "acc: 0.9666666666666667\n",
      "Epoch 177, loss:0.05244684126228094\n",
      "acc: 0.9666666666666667\n",
      "Epoch 178, loss:0.052296712063252926\n",
      "acc: 0.9666666666666667\n",
      "Epoch 179, loss:0.052147853188216686\n",
      "acc: 0.9666666666666667\n",
      "Epoch 180, loss:0.05200024601072073\n",
      "acc: 0.9666666666666667\n",
      "Epoch 181, loss:0.051853884011507034\n",
      "acc: 0.9666666666666667\n",
      "Epoch 182, loss:0.05170875322073698\n",
      "acc: 0.9666666666666667\n",
      "Epoch 183, loss:0.05156483966857195\n",
      "acc: 0.9666666666666667\n",
      "Epoch 184, loss:0.0514221154153347\n",
      "acc: 0.9666666666666667\n",
      "Epoch 185, loss:0.05128058698028326\n",
      "acc: 1.0\n",
      "Epoch 186, loss:0.05114021897315979\n",
      "acc: 1.0\n",
      "Epoch 187, loss:0.05100101325660944\n",
      "acc: 1.0\n",
      "Epoch 188, loss:0.050862946547567844\n",
      "acc: 1.0\n",
      "Epoch 189, loss:0.050726013258099556\n",
      "acc: 1.0\n",
      "Epoch 190, loss:0.050590199418365955\n",
      "acc: 1.0\n",
      "Epoch 191, loss:0.05045549385249615\n",
      "acc: 1.0\n",
      "Epoch 192, loss:0.05032187420874834\n",
      "acc: 1.0\n",
      "Epoch 193, loss:0.05018934607505798\n",
      "acc: 1.0\n",
      "Epoch 194, loss:0.050057871267199516\n",
      "acc: 1.0\n",
      "Epoch 195, loss:0.04992745816707611\n",
      "acc: 1.0\n",
      "Epoch 196, loss:0.04979808535426855\n",
      "acc: 1.0\n",
      "Epoch 197, loss:0.04966975096613169\n",
      "acc: 1.0\n",
      "Epoch 198, loss:0.049542431719601154\n",
      "acc: 1.0\n",
      "Epoch 199, loss:0.049416122026741505\n",
      "acc: 1.0\n",
      "Epoch 200, loss:0.04929081350564957\n",
      "acc: 1.0\n",
      "Epoch 201, loss:0.049166481010615826\n",
      "acc: 1.0\n",
      "Epoch 202, loss:0.04904312640428543\n",
      "acc: 1.0\n",
      "Epoch 203, loss:0.04892073478549719\n",
      "acc: 1.0\n",
      "Epoch 204, loss:0.048799299634993076\n",
      "acc: 1.0\n",
      "Epoch 205, loss:0.048678805120289326\n",
      "acc: 1.0\n",
      "Epoch 206, loss:0.04855924565345049\n",
      "acc: 1.0\n",
      "Epoch 207, loss:0.04844060633331537\n",
      "acc: 1.0\n",
      "Epoch 208, loss:0.048322876915335655\n",
      "acc: 1.0\n",
      "Epoch 209, loss:0.048206052742898464\n",
      "acc: 1.0\n",
      "Epoch 210, loss:0.048090117052197456\n",
      "acc: 1.0\n",
      "Epoch 211, loss:0.047975062392652035\n",
      "acc: 1.0\n",
      "Epoch 212, loss:0.047860884107649326\n",
      "acc: 1.0\n",
      "Epoch 213, loss:0.047747562639415264\n",
      "acc: 1.0\n",
      "Epoch 214, loss:0.047635095193982124\n",
      "acc: 1.0\n",
      "Epoch 215, loss:0.047523475252091885\n",
      "acc: 1.0\n",
      "Epoch 216, loss:0.04741268791258335\n",
      "acc: 1.0\n",
      "Epoch 217, loss:0.047302727587521076\n",
      "acc: 1.0\n",
      "Epoch 218, loss:0.047193583101034164\n",
      "acc: 1.0\n",
      "Epoch 219, loss:0.04708524700254202\n",
      "acc: 1.0\n",
      "Epoch 220, loss:0.04697770997881889\n",
      "acc: 1.0\n",
      "Epoch 221, loss:0.04687096830457449\n",
      "acc: 1.0\n",
      "Epoch 222, loss:0.046765005216002464\n",
      "acc: 1.0\n",
      "Epoch 223, loss:0.04665982350707054\n",
      "acc: 1.0\n",
      "Epoch 224, loss:0.04655539710074663\n",
      "acc: 1.0\n",
      "Epoch 225, loss:0.04645173065364361\n",
      "acc: 1.0\n",
      "Epoch 226, loss:0.046348828822374344\n",
      "acc: 1.0\n",
      "Epoch 227, loss:0.04624665342271328\n",
      "acc: 1.0\n",
      "Epoch 228, loss:0.04614521749317646\n",
      "acc: 1.0\n",
      "Epoch 229, loss:0.046044507063925266\n",
      "acc: 1.0\n",
      "Epoch 230, loss:0.04594452120363712\n",
      "acc: 1.0\n",
      "Epoch 231, loss:0.04584524221718311\n",
      "acc: 1.0\n",
      "Epoch 232, loss:0.04574666451662779\n",
      "acc: 1.0\n",
      "Epoch 233, loss:0.045648785308003426\n",
      "acc: 1.0\n",
      "Epoch 234, loss:0.045551598072052\n",
      "acc: 1.0\n",
      "Epoch 235, loss:0.045455098152160645\n",
      "acc: 1.0\n",
      "Epoch 236, loss:0.04535926226526499\n",
      "acc: 1.0\n",
      "Epoch 237, loss:0.0452641062438488\n",
      "acc: 1.0\n",
      "Epoch 238, loss:0.04516960773617029\n",
      "acc: 1.0\n",
      "Epoch 239, loss:0.04507577046751976\n",
      "acc: 1.0\n",
      "Epoch 240, loss:0.04498256929218769\n",
      "acc: 1.0\n",
      "Epoch 241, loss:0.0448900181800127\n",
      "acc: 1.0\n",
      "Epoch 242, loss:0.0447981059551239\n",
      "acc: 1.0\n",
      "Epoch 243, loss:0.04470681585371494\n",
      "acc: 1.0\n",
      "Epoch 244, loss:0.044616155326366425\n",
      "acc: 1.0\n",
      "Epoch 245, loss:0.044526112265884876\n",
      "acc: 1.0\n",
      "Epoch 246, loss:0.04443667456507683\n",
      "acc: 1.0\n",
      "Epoch 247, loss:0.044347841292619705\n",
      "acc: 1.0\n",
      "Epoch 248, loss:0.044259607791900635\n",
      "acc: 1.0\n",
      "Epoch 249, loss:0.04417196661233902\n",
      "acc: 1.0\n",
      "Epoch 250, loss:0.04408490937203169\n",
      "acc: 1.0\n",
      "Epoch 251, loss:0.043998437002301216\n",
      "acc: 1.0\n",
      "Epoch 252, loss:0.04391254112124443\n",
      "acc: 1.0\n",
      "Epoch 253, loss:0.04382720869034529\n",
      "acc: 1.0\n",
      "Epoch 254, loss:0.04374244902282953\n",
      "acc: 1.0\n",
      "Epoch 255, loss:0.043658243492245674\n",
      "acc: 1.0\n",
      "Epoch 256, loss:0.043574586510658264\n",
      "acc: 1.0\n",
      "Epoch 257, loss:0.043491482734680176\n",
      "acc: 1.0\n",
      "Epoch 258, loss:0.04340892378240824\n",
      "acc: 1.0\n",
      "Epoch 259, loss:0.043326898477971554\n",
      "acc: 1.0\n",
      "Epoch 260, loss:0.043245406821370125\n",
      "acc: 1.0\n",
      "Epoch 261, loss:0.04316443484276533\n",
      "acc: 1.0\n",
      "Epoch 262, loss:0.04308399651199579\n",
      "acc: 1.0\n",
      "Epoch 263, loss:0.04300406761467457\n",
      "acc: 1.0\n",
      "Epoch 264, loss:0.04292464815080166\n",
      "acc: 1.0\n",
      "Epoch 265, loss:0.042845744639635086\n",
      "acc: 1.0\n",
      "Epoch 266, loss:0.04276734031736851\n",
      "acc: 1.0\n",
      "Epoch 267, loss:0.04268943704664707\n",
      "acc: 1.0\n",
      "Epoch 268, loss:0.04261201433837414\n",
      "acc: 1.0\n",
      "Epoch 269, loss:0.04253508988767862\n",
      "acc: 1.0\n",
      "Epoch 270, loss:0.04245864972472191\n",
      "acc: 1.0\n",
      "Epoch 271, loss:0.04238268081098795\n",
      "acc: 1.0\n",
      "Epoch 272, loss:0.042307197116315365\n",
      "acc: 1.0\n",
      "Epoch 273, loss:0.04223217722028494\n",
      "acc: 1.0\n",
      "Epoch 274, loss:0.04215762298554182\n",
      "acc: 1.0\n",
      "Epoch 275, loss:0.042083533480763435\n",
      "acc: 1.0\n",
      "Epoch 276, loss:0.04200989846140146\n",
      "acc: 1.0\n",
      "Epoch 277, loss:0.04193671885877848\n",
      "acc: 1.0\n",
      "Epoch 278, loss:0.04186398535966873\n",
      "acc: 1.0\n",
      "Epoch 279, loss:0.041791703552007675\n",
      "acc: 1.0\n",
      "Epoch 280, loss:0.041719852946698666\n",
      "acc: 1.0\n",
      "Epoch 281, loss:0.041648443788290024\n",
      "acc: 1.0\n",
      "Epoch 282, loss:0.04157746862620115\n",
      "acc: 1.0\n",
      "Epoch 283, loss:0.0415069255977869\n",
      "acc: 1.0\n",
      "Epoch 284, loss:0.04143680538982153\n",
      "acc: 1.0\n",
      "Epoch 285, loss:0.04136709775775671\n",
      "acc: 1.0\n",
      "Epoch 286, loss:0.04129782132804394\n",
      "acc: 1.0\n",
      "Epoch 287, loss:0.04122895281761885\n",
      "acc: 1.0\n",
      "Epoch 288, loss:0.04116049222648144\n",
      "acc: 1.0\n",
      "Epoch 289, loss:0.04109244514256716\n",
      "acc: 1.0\n",
      "Epoch 290, loss:0.04102479945868254\n",
      "acc: 1.0\n",
      "Epoch 291, loss:0.040957553312182426\n",
      "acc: 1.0\n",
      "Epoch 292, loss:0.040890694595873356\n",
      "acc: 1.0\n",
      "Epoch 293, loss:0.040824235416948795\n",
      "acc: 1.0\n",
      "Epoch 294, loss:0.04075816832482815\n",
      "acc: 1.0\n",
      "Epoch 295, loss:0.04069248307496309\n",
      "acc: 1.0\n",
      "Epoch 296, loss:0.04062717501074076\n",
      "acc: 1.0\n",
      "Epoch 297, loss:0.04056225065141916\n",
      "acc: 1.0\n",
      "Epoch 298, loss:0.04049770254641771\n",
      "acc: 1.0\n",
      "Epoch 299, loss:0.04043352045118809\n",
      "acc: 1.0\n",
      "Epoch 300, loss:0.04036971367895603\n",
      "acc: 1.0\n",
      "Epoch 301, loss:0.0403062729164958\n",
      "acc: 1.0\n",
      "Epoch 302, loss:0.040243194438517094\n",
      "acc: 1.0\n",
      "Epoch 303, loss:0.04018047172576189\n",
      "acc: 1.0\n",
      "Epoch 304, loss:0.040118112694472075\n",
      "acc: 1.0\n",
      "Epoch 305, loss:0.040056094992905855\n",
      "acc: 1.0\n",
      "Epoch 306, loss:0.03999444004148245\n",
      "acc: 1.0\n",
      "Epoch 307, loss:0.03993312967941165\n",
      "acc: 1.0\n",
      "Epoch 308, loss:0.039872155990451574\n",
      "acc: 1.0\n",
      "Epoch 309, loss:0.03981153201311827\n",
      "acc: 1.0\n",
      "Epoch 310, loss:0.039751249831169844\n",
      "acc: 1.0\n",
      "Epoch 311, loss:0.03969130013138056\n",
      "acc: 1.0\n",
      "Epoch 312, loss:0.0396316796541214\n",
      "acc: 1.0\n",
      "Epoch 313, loss:0.039572388865053654\n",
      "acc: 1.0\n",
      "Epoch 314, loss:0.03951342636719346\n",
      "acc: 1.0\n",
      "Epoch 315, loss:0.03945479495450854\n",
      "acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 316, loss:0.03939648391678929\n",
      "acc: 1.0\n",
      "Epoch 317, loss:0.039338487200438976\n",
      "acc: 1.0\n",
      "Epoch 318, loss:0.039280812721699476\n",
      "acc: 1.0\n",
      "Epoch 319, loss:0.03922344418242574\n",
      "acc: 1.0\n",
      "Epoch 320, loss:0.0391664020717144\n",
      "acc: 1.0\n",
      "Epoch 321, loss:0.03910965658724308\n",
      "acc: 1.0\n",
      "Epoch 322, loss:0.039053221233189106\n",
      "acc: 1.0\n",
      "Epoch 323, loss:0.03899709181860089\n",
      "acc: 1.0\n",
      "Epoch 324, loss:0.038941262755542994\n",
      "acc: 1.0\n",
      "Epoch 325, loss:0.03888573171570897\n",
      "acc: 1.0\n",
      "Epoch 326, loss:0.03883049497380853\n",
      "acc: 1.0\n",
      "Epoch 327, loss:0.038775559049099684\n",
      "acc: 1.0\n",
      "Epoch 328, loss:0.03872090857475996\n",
      "acc: 1.0\n",
      "Epoch 329, loss:0.03866655798628926\n",
      "acc: 1.0\n",
      "Epoch 330, loss:0.03861248027533293\n",
      "acc: 1.0\n",
      "Epoch 331, loss:0.03855870338156819\n",
      "acc: 1.0\n",
      "Epoch 332, loss:0.038505202159285545\n",
      "acc: 1.0\n",
      "Epoch 333, loss:0.03845197381451726\n",
      "acc: 1.0\n",
      "Epoch 334, loss:0.03839903464540839\n",
      "acc: 1.0\n",
      "Epoch 335, loss:0.03834636649116874\n",
      "acc: 1.0\n",
      "Epoch 336, loss:0.038293981458991766\n",
      "acc: 1.0\n",
      "Epoch 337, loss:0.0382418492808938\n",
      "acc: 1.0\n",
      "Epoch 338, loss:0.03819000208750367\n",
      "acc: 1.0\n",
      "Epoch 339, loss:0.03813841473311186\n",
      "acc: 1.0\n",
      "Epoch 340, loss:0.038087100721895695\n",
      "acc: 1.0\n",
      "Epoch 341, loss:0.03803604608401656\n",
      "acc: 1.0\n",
      "Epoch 342, loss:0.037985253147780895\n",
      "acc: 1.0\n",
      "Epoch 343, loss:0.03793471725657582\n",
      "acc: 1.0\n",
      "Epoch 344, loss:0.03788444306701422\n",
      "acc: 1.0\n",
      "Epoch 345, loss:0.03783441847190261\n",
      "acc: 1.0\n",
      "Epoch 346, loss:0.03778465325012803\n",
      "acc: 1.0\n",
      "Epoch 347, loss:0.037735133431851864\n",
      "acc: 1.0\n",
      "Epoch 348, loss:0.03768586600199342\n",
      "acc: 1.0\n",
      "Epoch 349, loss:0.03763684071600437\n",
      "acc: 1.0\n",
      "Epoch 350, loss:0.03758806735277176\n",
      "acc: 1.0\n",
      "Epoch 351, loss:0.03753953380510211\n",
      "acc: 1.0\n",
      "Epoch 352, loss:0.03749124752357602\n",
      "acc: 1.0\n",
      "Epoch 353, loss:0.037443197797983885\n",
      "acc: 1.0\n",
      "Epoch 354, loss:0.037395389284938574\n",
      "acc: 1.0\n",
      "Epoch 355, loss:0.037347817327827215\n",
      "acc: 1.0\n",
      "Epoch 356, loss:0.03730046981945634\n",
      "acc: 1.0\n",
      "Epoch 357, loss:0.03725336864590645\n",
      "acc: 1.0\n",
      "Epoch 358, loss:0.03720649564638734\n",
      "acc: 1.0\n",
      "Epoch 359, loss:0.0371598438359797\n",
      "acc: 1.0\n",
      "Epoch 360, loss:0.03711343230679631\n",
      "acc: 1.0\n",
      "Epoch 361, loss:0.03706723731011152\n",
      "acc: 1.0\n",
      "Epoch 362, loss:0.037021270487457514\n",
      "acc: 1.0\n",
      "Epoch 363, loss:0.03697552578523755\n",
      "acc: 1.0\n",
      "Epoch 364, loss:0.036930002737790346\n",
      "acc: 1.0\n",
      "Epoch 365, loss:0.036884698551148176\n",
      "acc: 1.0\n",
      "Epoch 366, loss:0.03683960996568203\n",
      "acc: 1.0\n",
      "Epoch 367, loss:0.036794742569327354\n",
      "acc: 1.0\n",
      "Epoch 368, loss:0.03675008565187454\n",
      "acc: 1.0\n",
      "Epoch 369, loss:0.036705642472952604\n",
      "acc: 1.0\n",
      "Epoch 370, loss:0.036661417689174414\n",
      "acc: 1.0\n",
      "Epoch 371, loss:0.03661739407107234\n",
      "acc: 1.0\n",
      "Epoch 372, loss:0.03657358651980758\n",
      "acc: 1.0\n",
      "Epoch 373, loss:0.03652997734025121\n",
      "acc: 1.0\n",
      "Epoch 374, loss:0.036486583296209574\n",
      "acc: 1.0\n",
      "Epoch 375, loss:0.03644338948652148\n",
      "acc: 1.0\n",
      "Epoch 376, loss:0.03640039125457406\n",
      "acc: 1.0\n",
      "Epoch 377, loss:0.03635759884491563\n",
      "acc: 1.0\n",
      "Epoch 378, loss:0.036315012723207474\n",
      "acc: 1.0\n",
      "Epoch 379, loss:0.03627261845394969\n",
      "acc: 1.0\n",
      "Epoch 380, loss:0.036230424884706736\n",
      "acc: 1.0\n",
      "Epoch 381, loss:0.03618842037394643\n",
      "acc: 1.0\n",
      "Epoch 382, loss:0.03614661889150739\n",
      "acc: 1.0\n",
      "Epoch 383, loss:0.036105002742260695\n",
      "acc: 1.0\n",
      "Epoch 384, loss:0.036063570994883776\n",
      "acc: 1.0\n",
      "Epoch 385, loss:0.03602234786376357\n",
      "acc: 1.0\n",
      "Epoch 386, loss:0.03598130447790027\n",
      "acc: 1.0\n",
      "Epoch 387, loss:0.03594044130295515\n",
      "acc: 1.0\n",
      "Epoch 388, loss:0.03589977044612169\n",
      "acc: 1.0\n",
      "Epoch 389, loss:0.035859288182109594\n",
      "acc: 1.0\n",
      "Epoch 390, loss:0.035818986129015684\n",
      "acc: 1.0\n",
      "Epoch 391, loss:0.0357788666151464\n",
      "acc: 1.0\n",
      "Epoch 392, loss:0.03573892544955015\n",
      "acc: 1.0\n",
      "Epoch 393, loss:0.035699172876775265\n",
      "acc: 1.0\n",
      "Epoch 394, loss:0.03565958794206381\n",
      "acc: 1.0\n",
      "Epoch 395, loss:0.035620187409222126\n",
      "acc: 1.0\n",
      "Epoch 396, loss:0.035580962896347046\n",
      "acc: 1.0\n",
      "Epoch 397, loss:0.03554190369322896\n",
      "acc: 1.0\n",
      "Epoch 398, loss:0.0355030307546258\n",
      "acc: 1.0\n",
      "Epoch 399, loss:0.03546432824805379\n",
      "acc: 1.0\n",
      "Epoch 400, loss:0.035425789188593626\n",
      "acc: 1.0\n",
      "Epoch 401, loss:0.035387431271374226\n",
      "acc: 1.0\n",
      "Epoch 402, loss:0.035349227488040924\n",
      "acc: 1.0\n",
      "Epoch 403, loss:0.03531120391562581\n",
      "acc: 1.0\n",
      "Epoch 404, loss:0.03527333587408066\n",
      "acc: 1.0\n",
      "Epoch 405, loss:0.035235647577792406\n",
      "acc: 1.0\n",
      "Epoch 406, loss:0.03519811248406768\n",
      "acc: 1.0\n",
      "Epoch 407, loss:0.03516074176877737\n",
      "acc: 1.0\n",
      "Epoch 408, loss:0.03512354847043753\n",
      "acc: 1.0\n",
      "Epoch 409, loss:0.03508649580180645\n",
      "acc: 1.0\n",
      "Epoch 410, loss:0.03504961170256138\n",
      "acc: 1.0\n",
      "Epoch 411, loss:0.03501289337873459\n",
      "acc: 1.0\n",
      "Epoch 412, loss:0.03497632220387459\n",
      "acc: 1.0\n",
      "Epoch 413, loss:0.03493991121649742\n",
      "acc: 1.0\n",
      "Epoch 414, loss:0.034903660882264376\n",
      "acc: 1.0\n",
      "Epoch 415, loss:0.034867556765675545\n",
      "acc: 1.0\n",
      "Epoch 416, loss:0.03483161609619856\n",
      "acc: 1.0\n",
      "Epoch 417, loss:0.034795822110027075\n",
      "acc: 1.0\n",
      "Epoch 418, loss:0.034760176204144955\n",
      "acc: 1.0\n",
      "Epoch 419, loss:0.03472468629479408\n",
      "acc: 1.0\n",
      "Epoch 420, loss:0.034689352847635746\n",
      "acc: 1.0\n",
      "Epoch 421, loss:0.0346541665494442\n",
      "acc: 1.0\n",
      "Epoch 422, loss:0.03461911669000983\n",
      "acc: 1.0\n",
      "Epoch 423, loss:0.034584223292768\n",
      "acc: 1.0\n",
      "Epoch 424, loss:0.034549472853541374\n",
      "acc: 1.0\n",
      "Epoch 425, loss:0.03451487049460411\n",
      "acc: 1.0\n",
      "Epoch 426, loss:0.034480403643101454\n",
      "acc: 1.0\n",
      "Epoch 427, loss:0.03444609325379133\n",
      "acc: 1.0\n",
      "Epoch 428, loss:0.03441191837191582\n",
      "acc: 1.0\n",
      "Epoch 429, loss:0.03437788737937808\n",
      "acc: 1.0\n",
      "Epoch 430, loss:0.03434399655088782\n",
      "acc: 1.0\n",
      "Epoch 431, loss:0.03431024355813861\n",
      "acc: 1.0\n",
      "Epoch 432, loss:0.034276632126420736\n",
      "acc: 1.0\n",
      "Epoch 433, loss:0.034243152011185884\n",
      "acc: 1.0\n",
      "Epoch 434, loss:0.03420981904491782\n",
      "acc: 1.0\n",
      "Epoch 435, loss:0.03417661041021347\n",
      "acc: 1.0\n",
      "Epoch 436, loss:0.034143541008234024\n",
      "acc: 1.0\n",
      "Epoch 437, loss:0.03411061130464077\n",
      "acc: 1.0\n",
      "Epoch 438, loss:0.03407781291753054\n",
      "acc: 1.0\n",
      "Epoch 439, loss:0.034045143984258175\n",
      "acc: 1.0\n",
      "Epoch 440, loss:0.034012612886726856\n",
      "acc: 1.0\n",
      "Epoch 441, loss:0.033980210311710835\n",
      "acc: 1.0\n",
      "Epoch 442, loss:0.033947932068258524\n",
      "acc: 1.0\n",
      "Epoch 443, loss:0.03391579119488597\n",
      "acc: 1.0\n",
      "Epoch 444, loss:0.03388377418741584\n",
      "acc: 1.0\n",
      "Epoch 445, loss:0.03385188849642873\n",
      "acc: 1.0\n",
      "Epoch 446, loss:0.0338201317936182\n",
      "acc: 1.0\n",
      "Epoch 447, loss:0.03378849336877465\n",
      "acc: 1.0\n",
      "Epoch 448, loss:0.033756991382688284\n",
      "acc: 1.0\n",
      "Epoch 449, loss:0.033725607208907604\n",
      "acc: 1.0\n",
      "Epoch 450, loss:0.03369435062631965\n",
      "acc: 1.0\n",
      "Epoch 451, loss:0.03366321278735995\n",
      "acc: 1.0\n",
      "Epoch 452, loss:0.033632200211286545\n",
      "acc: 1.0\n",
      "Epoch 453, loss:0.03360131476074457\n",
      "acc: 1.0\n",
      "Epoch 454, loss:0.033570543862879276\n",
      "acc: 1.0\n",
      "Epoch 455, loss:0.03353988891467452\n",
      "acc: 1.0\n",
      "Epoch 456, loss:0.0335093610920012\n",
      "acc: 1.0\n",
      "Epoch 457, loss:0.033478949684649706\n",
      "acc: 1.0\n",
      "Epoch 458, loss:0.03344866121187806\n",
      "acc: 1.0\n",
      "Epoch 459, loss:0.03341847378760576\n",
      "acc: 1.0\n",
      "Epoch 460, loss:0.033388419542461634\n",
      "acc: 1.0\n",
      "Epoch 461, loss:0.03335847891867161\n",
      "acc: 1.0\n",
      "Epoch 462, loss:0.03332865610718727\n",
      "acc: 1.0\n",
      "Epoch 463, loss:0.03329894132912159\n",
      "acc: 1.0\n",
      "Epoch 464, loss:0.03326933830976486\n",
      "acc: 1.0\n",
      "Epoch 465, loss:0.0332398540340364\n",
      "acc: 1.0\n",
      "Epoch 466, loss:0.03321048337966204\n",
      "acc: 1.0\n",
      "Epoch 467, loss:0.03318122215569019\n",
      "acc: 1.0\n",
      "Epoch 468, loss:0.03315207548439503\n",
      "acc: 1.0\n",
      "Epoch 469, loss:0.03312302799895406\n",
      "acc: 1.0\n",
      "Epoch 470, loss:0.0330941011197865\n",
      "acc: 1.0\n",
      "Epoch 471, loss:0.033065283205360174\n",
      "acc: 1.0\n",
      "Epoch 472, loss:0.03303657751530409\n",
      "acc: 1.0\n",
      "Epoch 473, loss:0.03300797753036022\n",
      "acc: 1.0\n",
      "Epoch 474, loss:0.03297947905957699\n",
      "acc: 1.0\n",
      "Epoch 475, loss:0.03295108862221241\n",
      "acc: 1.0\n",
      "Epoch 476, loss:0.032922808080911636\n",
      "acc: 1.0\n",
      "Epoch 477, loss:0.032894634176045656\n",
      "acc: 1.0\n",
      "Epoch 478, loss:0.03286655806005001\n",
      "acc: 1.0\n",
      "Epoch 479, loss:0.03283859509974718\n",
      "acc: 1.0\n",
      "Epoch 480, loss:0.03281072899699211\n",
      "acc: 1.0\n",
      "Epoch 481, loss:0.032782967668026686\n",
      "acc: 1.0\n",
      "Epoch 482, loss:0.03275531064718962\n",
      "acc: 1.0\n",
      "Epoch 483, loss:0.03272775840014219\n",
      "acc: 1.0\n",
      "Epoch 484, loss:0.03270029090344906\n",
      "acc: 1.0\n",
      "Epoch 485, loss:0.032672944478690624\n",
      "acc: 1.0\n",
      "Epoch 486, loss:0.03264568652957678\n",
      "acc: 1.0\n",
      "Epoch 487, loss:0.03261853288859129\n",
      "acc: 1.0\n",
      "Epoch 488, loss:0.032591480761766434\n",
      "acc: 1.0\n",
      "Epoch 489, loss:0.03256453014910221\n",
      "acc: 1.0\n",
      "Epoch 490, loss:0.032537668477743864\n",
      "acc: 1.0\n",
      "Epoch 491, loss:0.03251091158017516\n",
      "acc: 1.0\n",
      "Epoch 492, loss:0.03248424921184778\n",
      "acc: 1.0\n",
      "Epoch 493, loss:0.03245767951011658\n",
      "acc: 1.0\n",
      "Epoch 494, loss:0.03243120992556214\n",
      "acc: 1.0\n",
      "Epoch 495, loss:0.03240483207628131\n",
      "acc: 1.0\n",
      "Epoch 496, loss:0.03237855155020952\n",
      "acc: 1.0\n",
      "Epoch 497, loss:0.03235236695036292\n",
      "acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 498, loss:0.03232626710087061\n",
      "acc: 1.0\n",
      "Epoch 499, loss:0.03230027249082923\n",
      "acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x_data = datasets.load_iris().data\n",
    "y_data = datasets.load_iris().target\n",
    "\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(x_data)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y_data)\n",
    "tf.random.set_seed(116)\n",
    "\n",
    "x_train = x_data[:-30]\n",
    "y_train = y_data[:-30]\n",
    "x_test = x_data[-30:]\n",
    "y_test = y_data[-30:]\n",
    "\n",
    "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev = 0.1, seed = 1))\n",
    "b1 = tf.Variable(tf.random.truncated_normal([3], stddev = 0.1, seed = 1))\n",
    "\n",
    "x_train = tf.cast(x_train, dtype = tf.float32)\n",
    "x_test = tf.cast(x_test, dtype = tf.float32)\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "lr = 0.1\n",
    "loss_all_results = []\n",
    "loss_all = 0\n",
    "epoch = 500\n",
    "\n",
    "test_acc = []\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    for i, (x_train, y_train) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = tf.matmul(x_train, w1) + b1\n",
    "            y = tf.nn.softmax(y)\n",
    "            y_ = tf.one_hot(y_train, depth = 3)\n",
    "            loss = tf.reduce_mean(tf.square(y - y_))\n",
    "            loss_all += loss.numpy()\n",
    "        \n",
    "        grads = tape.gradient(loss, [w1, b1])\n",
    "        \n",
    "        w1.assign_sub(lr * grads[0])\n",
    "        b1.assign_sub(lr * grads[1])\n",
    "        \n",
    "    loss_all_results.append(loss_all / 4)\n",
    "    print(\"Epoch {}, loss:{}\".format(epoch, loss_all / 4))\n",
    "    loss_all = 0\n",
    "        \n",
    "    total_correct, total_num = 0, 0\n",
    "    for x_test, y_test in test_db:\n",
    "        y = tf.matmul(x_test, w1) + b1\n",
    "        y = tf.nn.softmax(y)\n",
    "        pred = tf.argmax(y, axis = 1)\n",
    "        pred = tf.cast(pred, dtype = y_test.dtype)\n",
    "        correct = tf.cast(tf.equal(pred, y_test), dtype = tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        total_correct += int(correct)\n",
    "        total_num += x_test.shape[0]\n",
    "        \n",
    "    acc = total_correct / total_num\n",
    "    test_acc.append(acc)\n",
    "    print(\"acc:\", acc)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(loss_all_results, label = \"$Loss$\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.plot(test_acc, label = \"$Accuracy$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
